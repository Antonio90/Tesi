\section{Codice}
\label{sec:codice}
L'elaborato di tesi, come detto in precedenza, è suddiviso in due distinti progetti: Sistema Distribuito e Blockchain Explorer. In questo capitolo, l'attenzione sarà focalizzata sulla reale implementazione, visualizzando e commentando righe di codice con maggiore interesse.
\\ Ogni applicazione Java ha un \textit{entry point} e cioè una funzione principale che viene richiamata all'avvio dell'applicativo. All'interno del sistema distribuito l'entry point è definita all'interno della classe \textit{App}. Questa classe ha il compito di:
\begin{itemize}
\item \textbf{Recupero Costanti}: Il primo passo che effettua l'applicazione è il recupero delle costanti definite all'interno di un file di \textit{properties}: \textit{bitcoin.properties}. Per leggere le costanti di questo file utilizza un metodo statico della classe \textit{PropertiesReader} chiamato \textit{readProperties}. Questo metodo [\ref{lst:prop}], prende in input un file di proprieties e restituisce una mappa chiave-valore con tutte le proprietà definite nel file.

\begin{lstlisting}[language=Java, label=lst:prop, caption={Metodo readProperties.}]
public static Map<String, String> readProperties(String propFileName){

	Map<String, String>  propMap = new HashMap<String,String>();
	Properties prop = new Properties();

	try {
	prop.load(PropertiesReader.class.getClassLoader()
	.getResourceAsStream(propFileName));

		for (String key : prop.stringPropertyNames()) {
			String value = prop.getProperty(key);
			propMap.put(key, value);
		}

	} catch (IOException e) {

		e.printStackTrace();

	}

	return propMap;
}
\end{lstlisting}

\item \textbf{Inizializzare Spark e connessione con Bitcoind}: Caricate le costanti in una mappa, non resta che inizializzare Spark. Per fare ciò, viene creato un oggetto di tipo \textit{JavaStreamingContext} [\ref{lst:intiSpark}].

\begin{lstlisting}[language=Java, label=lst:intiSpark, caption={Inizializzazione Spark Streaming.}]
JavaStreamingContext streamingContext = new JavaStreamingContext(sparkConf, new Duration(2000));
\end{lstlisting}

Questa classe si occupa di creare il contesto streaming di Spark secondo le configurazioni presenti nell'oggetto \textit{sparkConf} e di creare Job eseguiti ogni due secondi. All'oggetto \textit{sparkConf} viene dato un nome che lo identifica in caso di più Job, impostato il tipo di cluster da creare ed infine l'URL di connessione col database Neo4j [\ref{lst:sparkConf}].

\begin{lstlisting}[language=Java, label=lst:sparkConf, caption={Creazione oggetto sparkConf.}]
SparkConf sparkConf = new SparkConf().setAppName(propMap.get("appSparkName"));
					
if (!sparkConf.contains("spark.master")) {
	/*local[K] (Run Spark locally with K threads, 
	usually k is set up to match the number of 
	cores on your machine)*/
	sparkConf.setMaster("local[2]"); 
}

/**
 * Configuration of Neo4j
 * */
sparkConf.set("spark.neo4j.bolt.url", neo4jConnectionUrl);
\end{lstlisting}

Una volta che il contesto di Spark è stato creato, non resta che creare il collegamento tra il Sistema Distribuito e Bitcoind. Questa operazione viene fatta tramite l'utilizzo della libreria \textit{spark-streaming-zeromq} di Apache Bahir. La libreria in questione permette di creare una connessione tra una coda ZMQ (utilizzata da Bitcoind) e Spark [\ref{lst:bahir}].

\begin{lstlisting}[language=Java, label=lst:bahir, caption={Metodo della libreria Spark Streaming ZeroMQ.}]
JavaDStream<byte[]> lines = ZeroMQUtils.createStream(streamingContext, host, subscribe, bytesToObjects );
\end{lstlisting}

Il metodo \textit{createStream} associa, quindi, la ricezione dei dati ad una funzione di callback da richiamare per gestire i dati. In questo caso, la funzione in questione è \textit{bytesToObjects} \ref{lst:bytesToObjects}, che estrae i byte provenienti dalla coda di Bitcoind e li trasforma in oggetti parallelizzati: \textit{JavaDStream}.

\begin{lstlisting}[language=Java, label=lst:bytesToObjects, caption={Funzione bytesToObjects.}]
Function<byte[][], Iterable<byte[]>> bytesToObjects = new Function<byte[][], Iterable<byte[]>>() {
            @Override
            public Iterable<byte[]> call(byte[][] bytes) throws Exception {
                Iterable iterable = Arrays.asList(bytes[0]);
                return iterable;
            }
        };
\end{lstlisting}

\item \textbf{Salvare i dati nell'HDFS}: Ottenuti i byte provenienti da Bitcoind può partire l'elaborazione. Il primo step da effettuare è il salvataggio sul filesystem distribuito Hadoop. Questo framework, precedentemente citato, si occupa della storicizzazione distribuita dei dati. Spark, abituato a lavorare su architetture distribuite, ha al suo interno metodi nativi che permettono di salvare dati in Hadoop. Infatti è bastato chiamare il metodo \textit{saveAsTextFile} della classe \textit{RDD} per salvare i dati all'interno del filesystem.

\begin{lstlisting}[language=Java, label=lst:hadoop, caption={Salvataggio Bytes su HDFS.}]
lines.foreachRDD((bytes, time)-> {

	List<byte[]> blockAsByte = bytes.collect();
	if (!blockAsByte.isEmpty()) {
		bytes.coalesce(1).saveAsTextFile(hadoopHdfs + File.separator + "blocks" + File.separator);
	}

});
\end{lstlisting}

Nel listato [\ref{lst:hadoop}] per ogni RDD, viene salvato un file di testo sul filesystem di Hadoop.

\item \textbf{Lanciare il job di Spark}: Terminate le operazioni preliminari, Spark è pronto per eseguire il Job. 
\\Tutti i dati, come visto nel listato \ref{lst:bahir}, sono contenuti in un oggetto chiamato \textit{lines}. Questo oggetto contiene la rappresentazione in byte dei blocchi provenienti da bitcoin, parallelizzati sui vari nodi del cluster. Il primo stepo da eseguire dunque, è la trasformazione da byte in oggetto.
\begin{itemize}
\item \textbf{Trasformare i Byte in oggetti}: La trasformazione di una sequenza di byte in blocco è demandata a \textit{Bitcoinj}. Questa libreria, creata da Google, è usata per lavorare con il protocollo Bitcoin. Può mantenere un portafoglio, inviare/ricevere transazioni senza bisogno di una copia locale di Bitcoin Core e ha molte altre funzionalità avanzate \cite{bitcoinj:home}. Implementato in Java, offre oggetti e metodi per gestire al meglio i dati provenienti da Bitcoind. 
\\ Nel Job di Spark, quindi, la trasformazione dei byte è fatta dal metodo \textit{blockMakerFromBytes} di \textit{BlockTestNetManager}. Il metodo in questione, partendo da un array di byte restituisce un oggetto che prende il nome di \textit{Block}. Il listato \ref{lst:bitcoindj} mostra la trasformazione dei byte provenienti di Bitcoin ad oggetto Block. 
\begin{lstlisting}[language=Java, label=lst:bitcoindj, caption={Array di Byte trasformato in oggetto Block.}]
BlockTestNetManager blockManager = new BlockTestNetManager();
Block block = blockManager.blockMakerFromBytes(blockAsByte);
\end{lstlisting}
\item \textbf{Salvare in Neo4j}: Per il recupero e l'analisi le transazioni sono storicizzate nel database a grafi Neo4j. Dall'oggetto \textit{block} precedentemente inizializzato, quindi, vengono recuperate tutte le transazioni tramite il metodo \textit{getTransactions()}. Il metodo restituisce una lista di transazioni (\textit{Transaction}) associate al blocco che il Job salva nel database.
\begin{lstlisting}[language=Java, label=lst:saveNeo, caption={Prelievo transazioni e salvataggio in Neo4j.}]
for (int i = 0; i < block.getTransactions().size(); i++) {
	BitcoinTransaction bTx = new BitcoinTransaction(tx.getHashAsString(),
                                block.getHashAsString(), tx.getInputs(), tx.getOutputs(),
                                i, new Date());
    Transaction tx = block.getTransactions().get(i);
    for(TransactionDBOutput tOut : bTx.getValidReceiver()) {
		log.info("#### Saving transaction in Neo4j ####");
        neo4jManager.createORupdate(Constants.NODE_LABEL, String.join(Constants.STRING_DELIMITER, bTx.getValidSender()), Constants.NODE_LABEL, tOut.getHash(),Constants.RELATIONS_LABEL, Constants.TYPE_OF_MONEY, Double.toString(tOut.getValue()),bTx.getHash() ,bTx.getBlockHash(), df.format(bTx.getReceivedTime()));
	}
}
\end{lstlisting}
Come si vede nel listato \ref{lst:saveNeo}, le transazioni recuperate dal blocco vengono nuovamente trasformate, per comodità, in un oggetto creato ad-hoc \textit{BitcoinTransaction}. Da questo oggetto sono recuperate tutte le transazioni che hanno un valido destinatario (\textit{getValidReceiver()}) e per ognuna di esse viene effettuata una query Cypher \ref{lst:UpdateNeo} che crea o modifica i nodi nel database.

\begin{lstlisting}[language=Java, label=lst:UpdateNeo, caption={Metodo che crea o modifica una transazione in Neo4j.}]
public void createORupdate(String $fromLabel, String $hashFrom, String $toLabel, String $hashTo, String $relationLabel, String $relType, String $relValue, String $transactionHash, String $blockHash, String receivedDate){

	try (Session session = driver.session()) {
		String greeting = session.writeTransaction(new TransactionWork<String>(){

		@Override
		public String execute( Transaction tx ) {
			StatementResult result = 
			
			tx.run("Merge (a:" + $fromLabel + "{hash:'"+ $hashFrom + "'})\n" + "Merge (b:" + $toLabel + "{hash:'" + $hashTo + "'})\n" + "Merge (a)-[r:" + $relationLabel + " {type: '" + $relType + "', value: '" + $relValue + "', transactionHash:'" + $transactionHash +"',blockHash: '" + $blockHash + "', receivedTime:'" + receivedDate + "'}]->(b);", parameters( "","") );
			return "Relationship Saved";
		}

		});
	}
}
\end{lstlisting}
\item \textbf{Calcolo del PageRank}: L'analisi delle transazioni è un'operazione che richiede molta potenza di calcolo. Il sistema distribuito però, tramite il cluster di Spark, riesce a gestire al meglio queste situazioni garantendo alta affidabilità e rapidità d'esecuzione. Nel progetto di tesi, per mostrare la potenza di Spark è stata utilizzato l'algoritmo di PageRank applicato su milioni di nodi.

\begin{lstlisting}[language=Java, label=lst:GraphX, caption={Calcolo PageRank e salvataggio su Neo4j.}]
log.info("### Start PageRank calculation by Graphx ###");

Graph graph = Neo4jGraph.loadGraph(
streamingContext.sparkContext().sc(),
Constants.NODE_LABEL, ScalaUtils.convertListToSeq
(Arrays.asList(Constants.RELATIONS_LABEL)), Constants.NODE_LABEL);

Graph pageRankGraph = PageRank.run(graph,Constants.NUMBER_OF_PAGE_RANK_ITERATIONS,
Constants.RANDOM_RESET_PROBABILITY, 
stringClassTag, stringClassTag);

Neo4jGraph.saveGraph(streamingContext.sparkContext().sc(), pageRankGraph, Constants.NODE_PAGE_RANK_PROP, new Tuple2<String,String>(Constants.RELATIONS_RANK_LABEL, Constants.RELATIONSHIP_PAGE_RANK_PROP) , scala.Option.apply(new Tuple2<String,String>(Constants.NODE_RANK_LABEL, Constants.PAGE_RANK_REFERENCE_ID)), scala.Option.apply(new Tuple2<String,String>(Constants.NODE_RANK_LABEL, Constants.PAGE_RANK_REFERENCE_ID)), true, stringClassTag, stringClassTag);

\end{lstlisting}

Nel codice \ref{lst:GraphX} sono riportare le righe di codice che esegue il Job di spark sui nodi esistenti nel sistema. Il primo passo è quello di caricare in una struttura dati tutti i nodi salvati precedentemente. GraphX per questo scopo, mette a disposizione la classe \textit{Graph}.
\\ Caricati i nodi delle transazioni nella struttura dati di Spark viene invocato il metodo \textit{PageRank.run} che esegue il calcolo del PageRank su tutti i nodi del grafo. Questo metodo ritorna un nuovo grafo contenente, per ogni nodo, il valore del PageRank ottenuto.
\\ Terminata l'esecuzione del metodo, non resta che salvare il nuovo grafo ottenuto nella base dati per un impiego futuro.

\item \textbf{Pubblicazione dei dati}: L'ultima operazione del Job è quella di pubblicare i dati per essere fruiti dalle applicazioni in real-time. Il sistema distribuito quindi, riceve da Bitcoin le informazioni, le storicizza, le analizza e le rimette a disposizione per altri consumatori. Le righe di codice  \ref{lst:pubKafka}, mostrano come sono inviati i dati processati, sul topic \textit{transaction-topic} di Kafka. 

\begin{lstlisting}[language=Java, label=lst:pubKafka, caption={Invio dati a Kafka.}]
log.info("#### Sending data to kafka ####");
final Producer<Long, String> producer = KafkaProducerBuilder.createProducer(kafkaHost + ":" + kafkaPort, kafkaAppID );
try {
	log.info("Sending JSON: " + json);
	final ProducerRecord<Long, String> record =
			new ProducerRecord<>(kafkaTopic, new Random().nextLong(),
					json);
	RecordMetadata metadata = producer.send(record).get();
} finally {
	producer.flush();
	producer.close();
}
\end{lstlisting}

In particolare, in queste poche righe di codice, viene creato un oggetto KafkaProducer, il quale ha il compito di connettersi al \textit{kafkaHost} e di inviare, tramite il metodo \textit{send}, i dati sul topic settato nel \textit{ProducerRecord}. 

\end{itemize}
\end{itemize} 

Per visualizzare le elaborazioni del sistema distribuito, è stata creata una applicazione web. Blockchain Explorer infatti, è un sito web che mette a disposizione strumenti per controllare le ultime transazioni, i risultati dell'analisi e la navigazione dell'intera Blockchain. Il codice di questo applicativo può essere riassunto tramite una serie di funzioni:

\begin{itemize}
\item \textbf{Creazione del server}: NodeJs per servire le varie richieste, provenienti dai client, ha bisogno di creare un server. Il modulo che permette questa funzionalità è \textit{http}. Per questo elaborato però, il server è demandato alla libreria Express.js che ci facilita l'implementazione.

\begin{lstlisting}[language=Javascript, label=lst:serverNode, caption={Creazione server in NodeJS.}]
const app = express();
const port = process.env.PORT || 3000;

app.listen(port, function () {
    console.log('express-handlebars example server listening on: ' + port);
});
\end{lstlisting}
Il listato \ref{lst:serverNode} mostra come sia possibile creare un server in NodeJS. Nello specifico, viene creata una variabile chiamata \textit{app}, inizializzata ad \textit{express}, che contiene l'intero web server. L'applicazione con queste poche righe di codice è in grado di rispondere a migliaia di richieste HTTP da parte dei client.
\item \textbf{Associazione URL-callback}
\item \textbf{Prelievo dati da Kafka}
\item \textbf{Comunicazione tramite Websocket}
\item \textbf{Renderizzazione dei Grafi}
\item \textbf{Costruzione delle tabelle}
\end{itemize}