\subsection{Apache Spark}
\label{sec:spark}
Apache Spark è un framework di calcolo del cluster per l'elaborazione di dati su larga scala, progettato e implementato nel 2010 da un gruppo di ricercatori dell’Università di Berkeley a San Francisco \cite{spark:hadoop}. Questo progetto nasce dall’esigenza di migliorare le prestazioni dei sistemi distribuiti “MapReduce”. Per questo si sviluppa il concetto di Resilient Distributed Dataset (RDD), che è la teoria alla base del sistema Spark. Un RDD rappresenta un set di dati che è suddiviso in partizioni (Una tabella chiave-valore suddivisa in tante sotto-tabelle o un file diviso in tanti segmenti). Un RDD ha la proprietà di essere immutabile, cioè una volta creato non può essere cambiato se non creandone un altro. La creazione di un RDD avviene a partire dai dati su disco (presi da HDFS) o da altre fonti di dati. Una volta creato, un RDD può restare in memoria oppure può essere materializzato su disco. Ogni RDD è descritto da un set completo di metadati che consentono la ricostruzione di una delle sue partizioni in caso di fault. Spark nasce come un sistema per creare e gestire job di analisi basati su trasformazioni di RDD. Dato che gli RDD nascono e vivono in memoria, l’esecuzione di lavori iterativi, o che trasformano più volte un set di dati, sono immensamente più rapide di una sequenza di MapReduce; questo perchè il disco non viene mai (o quasi mai) impiegato nell’elaborazione.
\\Come detto in precedenza, Spark non utilizza MapReduce come motore di esecuzione; invece, utilizza il proprio runtime distribuito (DAG) per l’esecuzione di jobs su un cluster. Quando viene invocata un’azione su un RDD, viene creato un “job”. Un Directed Acyclic Graph o DAG è un grafo aciclico in cui ogni nodo è una partizione di RDD e ogni vertice è una trasformazione. 
\\A differenza di MapReduce, il motore DAG di Spark può processare pipeline arbitrarie di operatori e tradurle in un unico “job” per l'utente. Spark sta dimostrando di essere una buona piattaforma su cui costruire strumenti di analisi, e, anche a questo fine del progetto Apache, Spark include moduli per: machine learning (MLlib), elaborazione grafica (GraphX), elaborazione di stream (Spark Streaming) e SQL (Spark SQL) \cite{spark:hadoop}.
\\Spark Streaming è un'estensione dell'API Spark di base che consente l'elaborazione in streaming scalabile, ad alto throughput e con tolleranza agli errori dei flussi di dati in tempo reale. I dati possono provenire da molte fonti come Apache Kafka, che analizzeremo più avanti, Apache Flume, che è un servizio distribuito affidabile per raccogliere, aggregare e spostare in maniera efficiente grandi quantità di dati di log, Twitter o socket TCP e possono essere elaborati utilizzando algoritmi complessi espressi con funzioni di alto livello come map, reduce, join e window. Infine, i dati elaborati possono essere trasferiti a filesystem, database e dashboard live. Spark Streaming riceve streams di dati di input e li divide in batch, che vengono quindi elaborati dal motore Spark e da lì verrà generato il risultato, ossia lo stream finale \cite{spark:home}.

\subsubsection{Hadoop HDFS}
\label{sec:hadoop HDFS}

\subsubsection{Neo4j}
\label{sec:neo4j}

\subsubsection{Graphx (PageRank)}
\label{sec:graphx (PageRank)}
